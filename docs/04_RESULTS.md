# Результаты экспериментов

Краткая фиксация результатов по гипотезам из `03_HYPOTHESES.md`. Таблицы и графики можно выносить в отдельные файлы (например `04_RESULTS_grid.csv`) и ссылаться отсюда.

## Целевые скоры оригиналов (на тесте baseline/sample_submission.csv, 2405 строк)

| Источник | Скор | Как получен в оригинале |
|----------|------|--------------------------|
| Блокнот (`копия_блокнота__pancake_problem_.py`) | **89980** | Только **v4**, treshold=**2.6**, без beam |
| pancake_91584 (`pancake_91584_final_edit.py`) | **91584** | Только **baseline + beam** (gap, 128×128), без блокнота |
| Baseline (классический pancake sort) | ~158680 | — |

**Как воспроизвести в проекте:**

- Скор ~89980: `python run_best_score.py --mode notebook` (или `python main.py solve --method notebook --solver v4 --treshold 2.6`).
- Скор ~91584: `python run_best_score.py --mode beam` (или `python main.py solve --method beam`).
- Режимы `crossing-*` (блокнот+beam) — экспериментальные, не повторяют ни один оригинал.

---

## H1: Улучшение (рефакторинг и тесты)

| Метрика | До | После | Комментарий |
|---------|-----|-------|-------------|
| — | — | — | Заполняется после рефакторинга и прогона тестов. |

---

## H2: Блокнот + Beam search

| Конфигурация | Солвер блокнота | beam_width / depth | mean_gain vs baseline | solved | Время (пример) |
|--------------|------------------|---------------------|------------------------|--------|----------------|
| — | — | — | — | — | Заполняется после прогонов. |

---

## H3: Блокнот + pancake_91584 (пайплайн)

| Что измеряли | Результат |
|--------------|-----------|
| — | Заполняется после интеграции. |

---

## H4: Блокнот + Beam + 91584 (включая ML)

| Стратегия | Сумма шагов (пример) | Время | Примечание |
|-----------|----------------------|-------|------------|
| — | — | — | Заполняется после реализации. |

---

## RL (политика BC + опционально PG)

Пайплайн: `run_rl.py` (train → solve → evaluate → submit). Модели сохраняются в `runs/rl_models/policy_n_{n}.pt`.

По `runs/experiment_results.jsonl` на тесте `baseline/sample_submission.csv` (2405 строк) получено два прогона BC-политики:

| Прогон | score (RL) | baseline_total | gain_vs_baseline | improved_cases | worse_cases |
|--------|-----------:|---------------:|------------------:|---------------:|------------:|
| #1     | 165967     | 158680         | −7287             | 153            | 847         |
| #2     | **165863** | 158680         | **−7183**         | 163            | 818         |

Сводка:

- **Лучший текущий скор RL (BC)**: 165863 (хуже baseline на 7183 шага).
- RL даёт улучшения на части кейсов (163 из 2405 во втором прогоне), но чаще ухудшает (818 кейсов).
- По сравнению с notebook (≈89980) и beam (≈91584) RL отстаёт ещё сильнее.

**Вывод:** политика после одного лишь BC на полном тесте стабильно проигрывает жадному baseline. Для улучшения нужны PG (fine-tune), больше данных или иная архитектура; в текущем виде H_RL_policy считается проваленной.

**Команда полного цикла (обучение → решение → оценка → сабмит):**

```bash
python run_rl.py full --train --test baseline/sample_submission.csv --models runs/rl_models --out submission.csv --evaluate --submit
```

Без `--train` используются уже сохранённые модели; без `--evaluate`/`--submit` выполняются только solve или solve+submit.

---

*Обновлять по мере появления новых прогонов. Имеет смысл хранить сырые CSV с прогонов в отдельной папке (например `runs/`) и здесь давать сводки.*
