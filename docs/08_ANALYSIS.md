# Анализ результатов экспериментов и выводы по гипотезам

Этот документ — инструкция для агента (и человека): как по данным экспериментов делать выводы, какие гипотезы подтвердились, какие провалились, и как обновлять документацию.

## Источники данных

1. **`runs/experiment_results.jsonl`** — каждая строка один JSON с полями:
   - `timestamp`, `script` (main / run_rl), `command` (solve / evaluate), `method` (baseline / beam / notebook / rl / evaluate)
   - `test_path`, `out_path`, `score`, `n_rows`
   - при evaluate: `baseline_score`, `gain_vs_baseline`, `total_gain`, `improved_cases`, `worse_cases`
2. **`submission_stats.py`** — сводка по submission по n: `python submission_stats.py --submission <file> --test baseline/sample_submission.csv`
3. **`04_RESULTS.md`** — куда записывать сводные цифры (скоры по методам, таблицы).
4. **`03_HYPOTHESES.md`** — список гипотез (H1–H4, H_merge, H_grid, H_RL_*, и т.д.); по каждой нужно вынести вердикт.

## Порядок анализа (для агента)

1. **Прочитать лог экспериментов**
   - Открыть `runs/experiment_results.jsonl`. Если файла нет — эксперименты ещё не логировались или папка `runs/` не использовалась.
   - Разобрать последние записи: какие методы дали какой `score`, было ли сравнение с baseline (`baseline_score`, `gain_vs_baseline`).

2. **Сопоставить с бейзлайнами**
   - Целевые скоры из `04_RESULTS.md`: блокнот v4 → 89980, beam 128×128 → 91584, baseline ~158680.
   - По логу: скор метода X лучше/хуже/равен baseline/notebook/beam — зафиксировать в 04_RESULTS.

3. **Обновить `04_RESULTS.md`**
   - В соответствующих секциях (H1–H4, RL, merge, grid и т.д.) заполнить таблицы: метод, скор, число строк, при необходимости gain vs baseline, время.
   - Добавить строку с датой/источником (например: «прогон run_rl.py full от …»).

4. **Сделать выводы по гипотезам в `03_HYPOTHESES.md`**
   - Для каждой гипотезы, по которой есть данные, в конце блока гипотезы добавить (или обновить) итог в формате:
     - **Итог (по данным от …):** подтверждена / провалена / частично / требует проверки. Краткое обоснование (цифры из experiment_results или 04_RESULTS).
   - Примеры:
     - H_merge: «Итог: подтверждена — merge notebook + beam дал скор X, ниже чем только notebook (Y) и только beam (Z).»
     - H_RL_policy: «Итог: провалена — скор RL (BC) на полном тесте X, выше baseline Y; политика не превзошла baseline.»
     - H_grid: «Итог: требует проверки — грид не запускался, данных нет.»

5. **При необходимости предложить следующие шаги**
   - Если гипотеза провалена — записать в 04_RESULTS возможные причины и идеи (другие гиперпараметры, другой метод).
   - Если подтверждена — зафиксировать конфигурацию (параметры) в 04_RESULTS и в README/командах.

## Важно

- Выводы должны опираться на **фактические записи** в `runs/experiment_results.jsonl` и на сводки из `submission_stats` / evaluate, а не на предположения.
- Если по гипотезе данных ещё нет — писать «требует проверки» и не помечать как подтверждённую или проваленную.
- После каждого значимого прогона (run_rl.py full, main.py solve с разными method и т.д.) имеет смысл заново проходить шаги 1–4 и обновлять 04_RESULTS и 03_HYPOTHESES.
